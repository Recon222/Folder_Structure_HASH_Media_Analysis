#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Success Message Builder - Pure business logic for building success messages.

This service class contains all the business logic for constructing success messages
from operation results, completely separated from UI concerns. It accepts Result objects
directly and produces SuccessMessageData objects for UI consumption.
"""

from typing import Any, Dict, List, Optional, Union
from pathlib import Path
import logging

from core.result_types import (
    Result, FileOperationResult, ReportGenerationResult, ArchiveOperationResult
)
from core.services.success_message_data import (
    SuccessMessageData, QueueOperationData, HashOperationData, BatchOperationData,
    EnhancedBatchOperationData, CopyVerifyOperationData
)


class SuccessMessageBuilder:
    """
    Pure business logic service for building success messages.
    
    This class contains no UI dependencies and focuses solely on the business logic
    of constructing appropriate success messages from operation results.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def build_forensic_success_message(
        self,
        file_result: FileOperationResult,
        report_results: Optional[Dict[str, ReportGenerationResult]] = None,
        zip_result: Optional[ArchiveOperationResult] = None
    ) -> SuccessMessageData:
        """
        Build comprehensive forensic operation success message.
        
        Args:
            file_result: File copying operation results
            report_results: Dictionary of report generation results
            zip_result: ZIP archive creation results
            
        Returns:
            SuccessMessageData object ready for UI display
        """
        summary_lines = []
        
        # File operation summary
        summary_lines.append(f"âœ“ Copied {file_result.files_processed} files")
        
        # Performance summary (check metadata for duration if needed)
        duration = file_result.duration_seconds
        if duration == 0 and hasattr(file_result, 'metadata'):
            duration = file_result.metadata.get('duration_seconds', 0)
        
        if file_result.files_processed > 0 and duration > 0:
            perf_summary = self._build_performance_summary(file_result)
            summary_lines.append(perf_summary)
        
        # Report generation summary
        if report_results:
            report_summary = self._build_report_summary(report_results)
            if report_summary:
                summary_lines.extend(report_summary)
        
        # ZIP archive summary
        if zip_result and zip_result.success:
            zip_summary = self._build_zip_summary(zip_result)
            if zip_summary:
                summary_lines.append(zip_summary)
        
        # Extract output location
        output_location = self._extract_output_location(file_result)
        
        # Extract performance data
        perf_data = self._extract_performance_dict(file_result)
        
        return SuccessMessageData(
            title="Operation Complete!",
            summary_lines=summary_lines,
            output_location=output_location,
            celebration_emoji="âœ…",
            performance_data=perf_data,
            raw_data={
                'file_result': file_result,
                'report_results': report_results,
                'zip_result': zip_result
            }
        )
    
    def build_queue_save_success_message(
        self, 
        queue_data: QueueOperationData
    ) -> SuccessMessageData:
        """
        Build queue save success message.
        
        Args:
            queue_data: Queue operation data
            
        Returns:
            SuccessMessageData for queue save success
        """
        summary_lines = [
            f"âœ“ Saved {queue_data.job_count} jobs to queue file",
            f"ðŸ“„ File size: {queue_data.get_file_size_display()}",
            f"ðŸ“ Location: {queue_data.file_path.parent}",
            f"ðŸ“ Filename: {queue_data.file_path.name}"
        ]
        
        if queue_data.duration_seconds > 0:
            summary_lines.append(f"â±ï¸ Save time: {queue_data.duration_seconds:.2f} seconds")
        
        return SuccessMessageData(
            title="Queue Saved Successfully!",
            summary_lines=summary_lines,
            output_location=str(queue_data.file_path),
            celebration_emoji="ðŸ’¾",
            raw_data={'queue_data': queue_data}
        )
    
    def build_queue_load_success_message(
        self,
        queue_data: QueueOperationData
    ) -> SuccessMessageData:
        """
        Build queue load success message.
        
        Args:
            queue_data: Queue operation data
            
        Returns:
            SuccessMessageData for queue load success
        """
        summary_lines = [
            f"âœ“ Loaded {queue_data.job_count} jobs from queue file",
            f"ðŸ“„ File size: {queue_data.get_file_size_display()}"
        ]
        
        if queue_data.duplicate_jobs_skipped > 0:
            summary_lines.append(f"âš ï¸ Skipped {queue_data.duplicate_jobs_skipped} duplicate jobs")
        
        if queue_data.duration_seconds > 0:
            summary_lines.append(f"â±ï¸ Load time: {queue_data.duration_seconds:.2f} seconds")
        
        return SuccessMessageData(
            title="Queue Loaded Successfully!",
            summary_lines=summary_lines,
            output_location="Jobs added to current queue",
            celebration_emoji="ðŸ“‚",
            raw_data={'queue_data': queue_data}
        )
    
    def build_batch_success_message(
        self,
        batch_data: BatchOperationData
    ) -> SuccessMessageData:
        """
        Build batch processing success message.
        
        Args:
            batch_data: Batch operation results
            
        Returns:
            SuccessMessageData for batch completion
        """
        summary_lines = [
            f"âœ“ Total jobs: {batch_data.total_jobs}",
            f"âœ“ Successful: {batch_data.successful_jobs}"
        ]
        
        if batch_data.failed_jobs > 0:
            summary_lines.append(f"âœ— Failed: {batch_data.failed_jobs}")
        
        success_rate = batch_data.get_success_rate()
        summary_lines.append(f"ðŸ“Š Success Rate: {success_rate:.1f}%")
        
        if batch_data.processing_time_seconds > 0:
            summary_lines.append(f"â±ï¸ Total time: {batch_data.processing_time_seconds:.1f} seconds")
        
        # Choose appropriate emoji and title based on success rate
        if batch_data.is_complete_success:
            emoji = "ðŸŽ‰"
            title = "Batch Processing Complete!"
        elif success_rate >= 80:
            emoji = "âœ…"
            title = "Batch Processing Complete!"
        else:
            emoji = "âš ï¸"
            title = "Batch Processing Finished with Issues"
        
        return SuccessMessageData(
            title=title,
            summary_lines=summary_lines,
            celebration_emoji=emoji,
            raw_data={'batch_data': batch_data}
        )
    
    def build_hash_verification_success_message(
        self,
        hash_data: HashOperationData
    ) -> SuccessMessageData:
        """
        Build hash verification success message.
        
        Args:
            hash_data: Hash operation results
            
        Returns:
            SuccessMessageData for hash verification success
        """
        summary_lines = [
            f"âœ“ Verified {hash_data.files_verified} files",
            f"ðŸ”’ Algorithm: {hash_data.hash_algorithm}",
            f"â±ï¸ Verification time: {hash_data.verification_time_seconds:.1f} seconds"
        ]
        
        if hash_data.failed_verifications > 0:
            success_rate = hash_data.get_success_rate()
            summary_lines.append(f"âš ï¸ Failed verifications: {hash_data.failed_verifications}")
            summary_lines.append(f"ðŸ“Š Success rate: {success_rate:.1f}%")
        
        if hash_data.csv_file_path:
            summary_lines.append(f"ðŸ“„ CSV report: {hash_data.csv_file_path.name}")
        
        return SuccessMessageData(
            title="Hash Verification Complete!",
            summary_lines=summary_lines,
            output_location=str(hash_data.csv_file_path) if hash_data.csv_file_path else None,
            celebration_emoji="ðŸ”’",
            raw_data={'hash_data': hash_data}
        )
    
    def build_copy_verify_success_message(
        self,
        copy_data: CopyVerifyOperationData
    ) -> SuccessMessageData:
        """
        Build copy & verify operation success message.
        
        Args:
            copy_data: Copy & verify operation results
            
        Returns:
            SuccessMessageData for copy & verify success display
        """
        summary_lines = []
        
        # Main operation summary
        if copy_data.files_failed_to_copy > 0:
            total_attempted = copy_data.files_copied + copy_data.files_failed_to_copy
            summary_lines.append(f"âš ï¸ Copied {copy_data.files_copied}/{total_attempted} files ({copy_data.get_size_display()})")
            summary_lines.append(f"âŒ {copy_data.files_failed_to_copy} files failed to copy")
        else:
            summary_lines.append(f"âœ… Successfully copied {copy_data.files_copied} files ({copy_data.get_size_display()})")
        
        # Hash verification summary
        if copy_data.hash_verification_enabled:
            summary_lines.append("")  # Add spacing
            if copy_data.files_with_hash_mismatch > 0:
                summary_lines.append(f"âš ï¸ Hash Verification Issues:")
                summary_lines.append(f"  â€¢ {copy_data.files_with_hash_mismatch} files had hash mismatches")
                verified_count = copy_data.files_copied - copy_data.files_with_hash_mismatch
                if verified_count > 0:
                    summary_lines.append(f"  â€¢ {verified_count} files verified successfully")
            else:
                summary_lines.append(f"âœ… All file hashes verified successfully")
        
        # Performance summary
        if copy_data.operation_time_seconds > 0 and copy_data.bytes_processed > 0:
            summary_lines.append("")  # Add spacing
            summary_lines.append("ðŸ“Š Performance Summary:")
            summary_lines.append(f"  â€¢ Time: {copy_data.operation_time_seconds:.1f} seconds")
            
            if copy_data.average_speed_mbps > 0:
                summary_lines.append(f"  â€¢ Average speed: {copy_data.average_speed_mbps:.1f} MB/s")
            
            if copy_data.peak_speed_mbps > 0 and copy_data.peak_speed_mbps != copy_data.average_speed_mbps:
                summary_lines.append(f"  â€¢ Peak speed: {copy_data.peak_speed_mbps:.1f} MB/s")
        
        # CSV report info
        if copy_data.csv_generated and copy_data.csv_path:
            summary_lines.append("")  # Add spacing
            summary_lines.append(f"ðŸ“„ CSV report saved: {copy_data.csv_path.name}")
        
        # Operation details
        if copy_data.preserve_structure:
            summary_lines.append("")  # Add spacing
            summary_lines.append("â„¹ï¸ Folder structure preserved during copy")
        
        # Determine title and emoji based on results
        if copy_data.has_issues:
            if copy_data.files_failed_to_copy > 0:
                title = "Copy Operation Completed with Errors"
                emoji = "âš ï¸"
            else:
                title = "Copy Complete - Hash Verification Issues"
                emoji = "âš ï¸"
        else:
            title = "Copy & Verify Complete!"
            emoji = "âœ…"
        
        # Build performance data dictionary for compatibility
        perf_data = {}
        if copy_data.operation_time_seconds > 0:
            perf_data = {
                'files_processed': copy_data.files_copied,
                'bytes_processed': copy_data.bytes_processed,
                'total_time_seconds': copy_data.operation_time_seconds,
                'average_speed_mbps': copy_data.average_speed_mbps,
                'peak_speed_mbps': copy_data.peak_speed_mbps,
                'total_size_mb': copy_data.bytes_processed / (1024 * 1024)
            }
        
        return SuccessMessageData(
            title=title,
            summary_lines=summary_lines,
            output_location=str(copy_data.csv_path.parent) if copy_data.csv_path else None,
            celebration_emoji=emoji,
            performance_data=perf_data,
            raw_data={'copy_data': copy_data}
        )
    
    def build_enhanced_batch_success_message(
        self,
        enhanced_batch_data: EnhancedBatchOperationData
    ) -> SuccessMessageData:
        """
        Build comprehensive batch success message with aggregate data.
        
        Args:
            enhanced_batch_data: Enhanced batch operation results with aggregate metrics
            
        Returns:
            SuccessMessageData for rich batch completion display
        """
        summary_lines = []
        
        # Job summary with rich context
        if enhanced_batch_data.failed_jobs > 0:
            summary_lines.append(f"âœ“ Completed {enhanced_batch_data.successful_jobs}/{enhanced_batch_data.total_jobs} jobs")
            summary_lines.append(f"âš ï¸ {enhanced_batch_data.failed_jobs} job(s) failed")
        else:
            summary_lines.append(f"âœ“ All {enhanced_batch_data.total_jobs} jobs completed successfully")
        
        # Aggregate performance summary (if we have file processing data)
        if enhanced_batch_data.total_files_processed > 0:
            summary_lines.append("")  # Spacing
            summary_lines.append("ðŸ“Š Aggregate Performance Summary:")
            summary_lines.append(f"Files: {enhanced_batch_data.total_files_processed} total files processed")
            
            total_gb = enhanced_batch_data.get_total_size_gb()
            if total_gb >= 1.0:
                summary_lines.append(f"Size: {total_gb:.1f} GB across all jobs")
            else:
                total_mb = enhanced_batch_data.total_bytes_processed / (1024**2)
                summary_lines.append(f"Size: {total_mb:.1f} MB across all jobs")
            
            processing_minutes = enhanced_batch_data.get_processing_time_minutes()
            if processing_minutes >= 1.0:
                summary_lines.append(f"Time: {processing_minutes:.1f} minutes total processing")
            else:
                summary_lines.append(f"Time: {enhanced_batch_data.processing_time_seconds:.1f} seconds total processing")
            
            if enhanced_batch_data.aggregate_speed_mbps > 0:
                summary_lines.append(f"Average Speed: {enhanced_batch_data.aggregate_speed_mbps:.1f} MB/s overall")
            
            if enhanced_batch_data.peak_speed_mbps > 0 and enhanced_batch_data.peak_speed_job_name:
                summary_lines.append(f"Peak Speed: {enhanced_batch_data.peak_speed_mbps:.1f} MB/s ({enhanced_batch_data.peak_speed_job_name})")
        
        # Aggregate report summary
        if enhanced_batch_data.total_reports_generated > 0:
            summary_lines.append("")  # Spacing
            summary_lines.append(f"âœ“ Generated {enhanced_batch_data.total_reports_generated} reports across all jobs")
            
            for report_type, count in enhanced_batch_data.report_breakdown.items():
                display_name = self._get_report_display_name(report_type)
                summary_lines.append(f"  â€¢ {count} {display_name}{'s' if count > 1 else ''}")
            
            if enhanced_batch_data.total_report_size_bytes > 0:
                total_report_mb = enhanced_batch_data.get_total_report_size_mb()
                summary_lines.append(f"  â€¢ Total report size: {total_report_mb:.1f} MB")
        
        # ZIP summary
        if enhanced_batch_data.total_zip_archives > 0:
            total_zip_gb = enhanced_batch_data.get_total_zip_size_gb()
            if total_zip_gb >= 1.0:
                summary_lines.append(f"âœ“ Created {enhanced_batch_data.total_zip_archives} ZIP archives ({total_zip_gb:.1f} GB total)")
            else:
                total_zip_mb = enhanced_batch_data.total_zip_size_bytes / (1024**2)
                summary_lines.append(f"âœ“ Created {enhanced_batch_data.total_zip_archives} ZIP archives ({total_zip_mb:.1f} MB total)")
        
        # Success rate and failed job details
        success_rate = enhanced_batch_data.get_success_rate()
        summary_lines.append("")  # Spacing
        summary_lines.append(f"ðŸ“Š Success Rate: {success_rate:.1f}%")
        
        # Show failed job names if any
        if enhanced_batch_data.failed_job_summaries:
            summary_lines.append("")  # Spacing
            for failure in enhanced_batch_data.failed_job_summaries[:3]:  # Limit to first 3
                summary_lines.append(f"âš ï¸ {failure}")
            if len(enhanced_batch_data.failed_job_summaries) > 3:
                remaining = len(enhanced_batch_data.failed_job_summaries) - 3
                summary_lines.append(f"âš ï¸ ...and {remaining} more failure{'s' if remaining > 1 else ''}")
        
        # Choose appropriate professional emoji and title
        if success_rate == 100:
            emoji = "âœ…"
            title = "Batch Processing Complete!"
        elif success_rate >= 90:
            emoji = "âš ï¸"
            title = "Batch Processing Complete with Minor Issues"
        elif success_rate >= 70:
            emoji = "âš ï¸"
            title = "Batch Processing Complete with Some Issues"
        else:
            emoji = "âŒ"
            title = "Batch Processing Complete with Significant Issues"
        
        # Output location
        output_location = None
        if enhanced_batch_data.batch_output_directories:
            if len(enhanced_batch_data.batch_output_directories) == 1:
                output_location = str(enhanced_batch_data.batch_output_directories[0])
            else:
                output_location = f"Multiple locations ({len(enhanced_batch_data.batch_output_directories)} jobs)"
        
        return SuccessMessageData(
            title=title,
            summary_lines=summary_lines,
            output_location=output_location,
            celebration_emoji=emoji,
            raw_data={'enhanced_batch_data': enhanced_batch_data}
        )
    
    # Private helper methods
    
    def _build_performance_summary(self, file_result: FileOperationResult) -> str:
        """Build performance summary from file operation result."""
        # Check for duration in metadata if not in main attributes
        duration = file_result.duration_seconds
        if duration == 0 and hasattr(file_result, 'metadata'):
            duration = file_result.metadata.get('duration_seconds', 0)
        
        if not (file_result.files_processed > 0 and duration > 0):
            return ""
        
        # Calculate speed if not available
        avg_speed = file_result.average_speed_mbps
        if avg_speed == 0 and duration > 0:
            avg_speed = (file_result.bytes_processed / (1024 * 1024)) / duration
        
        lines = [
            f"Files: {file_result.files_processed}",
            f"Size: {file_result.bytes_processed / (1024 * 1024):.1f} MB",
            f"Time: {duration:.1f} seconds",
            f"Average Speed: {avg_speed:.1f} MB/s"
        ]
        
        # Add peak speed if available
        if hasattr(file_result, 'peak_speed_mbps') and file_result.peak_speed_mbps:
            lines.append(f"Peak Speed: {file_result.peak_speed_mbps:.1f} MB/s")
        
        # Add mode if available
        if hasattr(file_result, 'optimization_mode'):
            lines.append(f"Mode: {file_result.optimization_mode}")
        
        return "ðŸ“Š Performance Summary:\n" + "\n".join(lines)
    
    def _build_report_summary(
        self, 
        report_results: Dict[str, ReportGenerationResult]
    ) -> List[str]:
        """Build report generation summary from results."""
        successful_reports = []
        
        for report_type, result in report_results.items():
            if result.success:
                # Get file size in KB
                file_size_kb = result.file_size_bytes / 1024 if result.file_size_bytes > 0 else 0
                report_name = self._get_report_display_name(report_type)
                
                if file_size_kb > 0:
                    successful_reports.append(f"  â€¢ {report_name} ({file_size_kb:.0f} KB)")
                else:
                    successful_reports.append(f"  â€¢ {report_name}")
        
        if successful_reports:
            return [f"âœ“ Generated {len(successful_reports)} reports"] + successful_reports
        
        return []
    
    def _build_zip_summary(self, zip_result: ArchiveOperationResult) -> str:
        """Build ZIP archive summary from results."""
        if not zip_result.success or not hasattr(zip_result, 'archives_created'):
            return ""
        
        # archives_created is an int, not a list
        archive_count = zip_result.archives_created if zip_result.archives_created else 1
        total_size_mb = zip_result.total_compressed_size / (1024 * 1024) if zip_result.total_compressed_size else 0
        
        if total_size_mb > 0:
            return f"âœ“ Created {archive_count} ZIP archive(s) ({total_size_mb:.1f} MB)"
        else:
            return f"âœ“ Created {archive_count} ZIP archive(s)"
    
    def _extract_output_location(self, file_result: FileOperationResult) -> Optional[str]:
        """Extract output location from file operation result."""
        if hasattr(file_result, 'output_directory') and file_result.output_directory:
            return str(file_result.output_directory)
        
        # Fallback: try to extract from file paths
        if hasattr(file_result, 'value') and file_result.value:
            if isinstance(file_result.value, dict):
                for path_data in file_result.value.values():
                    if isinstance(path_data, dict) and 'dest_path' in path_data:
                        return str(Path(path_data['dest_path']).parent)
        
        return None
    
    def _extract_performance_dict(self, file_result: FileOperationResult) -> Dict[str, Any]:
        """Extract performance data as dictionary for compatibility."""
        if not (file_result.files_processed > 0 and file_result.duration_seconds > 0):
            return {}
        
        result = {
            'files_processed': file_result.files_processed,
            'bytes_processed': file_result.bytes_processed,
            'total_time_seconds': file_result.duration_seconds,
            'average_speed_mbps': file_result.average_speed_mbps,
            'total_size_mb': file_result.bytes_processed / (1024 * 1024),
            'peak_speed_mbps': getattr(file_result, 'peak_speed_mbps', file_result.average_speed_mbps),
            'mode': getattr(file_result, 'optimization_mode', 'Balanced')
        }
        return result
    
    def _get_report_display_name(self, report_type: str) -> str:
        """Convert report type to display-friendly name."""
        display_names = {
            'time_offset': 'Time Offset Report',
            'technician_log': 'Technician Log',
            'hash_csv': 'Hash Verification CSV',
            'upload_log': 'Upload Log',
            'processing_summary': 'Processing Summary'
        }
        
        return display_names.get(report_type, report_type.replace('_', ' ').title())